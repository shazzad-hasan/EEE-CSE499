{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('D:/CSE499/osic-pulmonary-fibrosis-progression/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>Weeks</th>\n",
       "      <th>FVC</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SmokingStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>-4</td>\n",
       "      <td>2315</td>\n",
       "      <td>58.253649</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>5</td>\n",
       "      <td>2214</td>\n",
       "      <td>55.712129</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>7</td>\n",
       "      <td>2061</td>\n",
       "      <td>51.862104</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>9</td>\n",
       "      <td>2144</td>\n",
       "      <td>53.950679</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>11</td>\n",
       "      <td>2069</td>\n",
       "      <td>52.063412</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Patient  Weeks   FVC    Percent  Age   Sex SmokingStatus\n",
       "0  ID00007637202177411956430     -4  2315  58.253649   79  Male     Ex-smoker\n",
       "1  ID00007637202177411956430      5  2214  55.712129   79  Male     Ex-smoker\n",
       "2  ID00007637202177411956430      7  2061  51.862104   79  Male     Ex-smoker\n",
       "3  ID00007637202177411956430      9  2144  53.950679   79  Male     Ex-smoker\n",
       "4  ID00007637202177411956430     11  2069  52.063412   79  Male     Ex-smoker"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ex-smoker', 'Never smoked', 'Currently smokes'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.SmokingStatus.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tab(df):\n",
    "    \"raturn an array which contains each patient normalized age, sex and smoking status\"\n",
    "    vector = [(df.Age.values[0] - 30) / 30] \n",
    "    \n",
    "    if df.Sex.values[0].lower() == 'male':\n",
    "       vector.append(0)\n",
    "    else:\n",
    "       vector.append(1)\n",
    "    \n",
    "    if df.SmokingStatus.values[0] == 'Never smoked':\n",
    "        vector.extend([0,0])\n",
    "    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n",
    "        vector.extend([1,1])\n",
    "    elif df.SmokingStatus.values[0] == 'Currently smokes':\n",
    "        vector.extend([0,1])\n",
    "    else:\n",
    "        vector.extend([1,0])\n",
    "    return np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156e2d9a2c634a528b0f0603a74b3045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Monir\\anaconda3\\envs\\rabbi36\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "A = {} \n",
    "TAB = {} \n",
    "P = [] \n",
    "for i, p in tqdm(enumerate(train.Patient.unique())): # i index, p patient id\n",
    "    sub = train.loc[train.Patient == p, :] # find all data (weeks, FVC, Percent, Age, Sex, SmokingStatus) of a unique patient\n",
    "    fvc = sub.FVC.values # fvc values of the patient during the follow-up\n",
    "    weeks = sub.Weeks.values # follow-up weeks\n",
    "    c = np.vstack([weeks, np.ones(len(weeks))]).T # create an array by the follow-up weeks of shape(len(weeks),2)\n",
    "    a, b = np.linalg.lstsq(c, fvc)[0] # least-square sol, a=gradient matrix, b=right hand matrix \n",
    "    \n",
    "    A[p] = a\n",
    "    TAB[p] = get_tab(sub)\n",
    "    P.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(path):\n",
    "    \"read DICOM dataset and return resize images of size (512,512,1)\"\n",
    "    d = pydicom.dcmread(path) # read and parse the CT scan images (in DICOM format)\n",
    "    resized_image = cv2.resize((d.pixel_array - d.RescaleIntercept) / (d.RescaleSlope * 1000), (512,512))\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class IGenerator(Sequence):\n",
    "    BAD_ID = ['ID00011637202177653955184', 'ID00052637202186188008618']\n",
    "    def __init__(self, keys, a, tab, batch_size):\n",
    "        \"key=patient, a=gradient matrix, tab=a particular patient's data\"\n",
    "        self.keys = [k for k in keys if k not in self.BAD_ID]\n",
    "        self.a = a\n",
    "        self.tab = tab\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.train_data = {}\n",
    "        for p in train.Patient.values:\n",
    "            self.train_data[p] = os.listdir(f'D:/CSE499/osic-pulmonary-fibrosis-progression/train/{p}/')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1000\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = []\n",
    "        a, tab = [], [] \n",
    "        keys = np.random.choice(self.keys, size = self.batch_size) # randomly chooses n=batch_size number of patients\n",
    "        for k in keys:\n",
    "            try:\n",
    "                i = np.random.choice(self.train_data[k], size=1)[0] # chooses some randomly images for kth patient\n",
    "                img = get_img(f'D:/CSE499/osic-pulmonary-fibrosis-progression/train/{k}/{i}') # resizes ith image of kth patient\n",
    "                x.append(img) # append kth patient's image data in the list x\n",
    "                a.append(self.a[k]) # append kth patient's gradinet in the list a\n",
    "                tab.append(self.tab[k]) # append kth patient's tabular data in the tab list\n",
    "            except:\n",
    "                print(k, i)\n",
    "       \n",
    "        x,a,tab = np.array(x), np.array(a), np.array(tab) # convert list to array\n",
    "        x = np.expand_dims(x, axis=-1) \n",
    "        return [x, tab] , a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, Activation, Flatten, Input, BatchNormalization, GlobalAveragePooling2D, GaussianNoise, \n",
    "    Add, Conv2D, AveragePooling2D, LeakyReLU, Concatenate \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "def get_efficientnet(model, shape):\n",
    "    models_dict = {\n",
    "        'b0': efn.EfficientNetB0(input_shape=shape,weights=None,include_top=False),\n",
    "        'b1': efn.EfficientNetB1(input_shape=shape,weights=None,include_top=False),\n",
    "        'b2': efn.EfficientNetB2(input_shape=shape,weights=None,include_top=False),\n",
    "        'b3': efn.EfficientNetB3(input_shape=shape,weights=None,include_top=False),\n",
    "        'b4': efn.EfficientNetB4(input_shape=shape,weights=None,include_top=False),\n",
    "        'b5': efn.EfficientNetB5(input_shape=shape,weights=None,include_top=False),\n",
    "        'b6': efn.EfficientNetB6(input_shape=shape,weights=None,include_top=False),\n",
    "        'b7': efn.EfficientNetB7(input_shape=shape,weights=None,include_top=False)\n",
    "    }\n",
    "    return models_dict[model]\n",
    "\n",
    "def build_model(shape=(512, 512, 1), model_class=None):\n",
    "    inp = Input(shape=shape)\n",
    "    base = get_efficientnet(model_class, shape)\n",
    "    x = base(inp)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    inp2 = Input(shape=(4,)) # indicates that the expected input will be batches of 4-dimensional vectors\n",
    "    x2 = GaussianNoise(0.2)(inp2) # to mitigate overfitting\n",
    "    x = Concatenate()([x, x2]) \n",
    "    x = Dropout(0.5)(x) \n",
    "    x = Dense(1)(x)\n",
    "    model = Model([inp, inp2] , x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512, 512, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "efficientnet-b7 (Model)         (None, 16, 16, 2560) 64096528    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2560)         0           efficientnet-b7[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 4)            0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2564)         0           global_average_pooling2d[0][0]   \n",
      "                                                                 gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2564)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2565        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 64,099,093\n",
      "Trainable params: 63,788,373\n",
      "Non-trainable params: 310,720\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MODEL_CLASS = 'b7'\n",
    "base_model = build_model(shape=(512, 512, 1), model_class=MODEL_CLASS)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCHS = 80\n",
    "LR = 0.001\n",
    "SAVE_BEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6164\n",
      "Epoch 00001: val_loss improved from inf to 4.61137, saving model to C:/Users/Monir/Documents/CSE499/models/EfficientNet/b7_80_epochs.h5\n",
      "32/32 [==============================] - 16s 512ms/step - loss: 4.6164 - val_loss: 4.6114 - lr: 0.0010\n",
      "Epoch 2/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8757\n",
      "Epoch 00002: val_loss did not improve from 4.61137\n",
      "32/32 [==============================] - 9s 284ms/step - loss: 4.8757 - val_loss: 15.7852 - lr: 0.0010\n",
      "Epoch 3/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7057\n",
      "Epoch 00003: val_loss did not improve from 4.61137\n",
      "32/32 [==============================] - 9s 285ms/step - loss: 4.7057 - val_loss: 201.9294 - lr: 0.0010\n",
      "Epoch 4/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.2883\n",
      "Epoch 00004: val_loss did not improve from 4.61137\n",
      "32/32 [==============================] - 9s 285ms/step - loss: 5.2883 - val_loss: 14.6836 - lr: 0.0010\n",
      "Epoch 5/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.5549\n",
      "Epoch 00005: val_loss improved from 4.61137 to 4.09126, saving model to C:/Users/Monir/Documents/CSE499/models/EfficientNet/b7_80_epochs.h5\n",
      "32/32 [==============================] - 25s 792ms/step - loss: 3.5549 - val_loss: 4.0913 - lr: 0.0010\n",
      "Epoch 6/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.4649\n",
      "Epoch 00006: val_loss did not improve from 4.09126\n",
      "32/32 [==============================] - 10s 301ms/step - loss: 3.4649 - val_loss: 15.1450 - lr: 0.0010\n",
      "Epoch 7/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0280\n",
      "Epoch 00007: val_loss did not improve from 4.09126\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 4.0280 - val_loss: 4.4634 - lr: 0.0010\n",
      "Epoch 8/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2212\n",
      "Epoch 00008: val_loss did not improve from 4.09126\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 4.2212 - val_loss: 135.5941 - lr: 0.0010\n",
      "Epoch 9/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 6.0175\n",
      "Epoch 00009: val_loss did not improve from 4.09126\n",
      "32/32 [==============================] - 9s 285ms/step - loss: 6.0175 - val_loss: 461.6105 - lr: 0.0010\n",
      "Epoch 10/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.4593\n",
      "Epoch 00010: val_loss did not improve from 4.09126\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "32/32 [==============================] - 9s 290ms/step - loss: 3.4593 - val_loss: 1324.3800 - lr: 0.0010\n",
      "Epoch 11/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.5648\n",
      "Epoch 00011: val_loss did not improve from 4.09126\n",
      "32/32 [==============================] - 9s 288ms/step - loss: 5.5648 - val_loss: 22.3203 - lr: 5.0000e-04\n",
      "Epoch 12/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1728\n",
      "Epoch 00012: val_loss did not improve from 4.09126\n",
      "32/32 [==============================] - 9s 285ms/step - loss: 4.1728 - val_loss: 17.3123 - lr: 5.0000e-04\n",
      "Epoch 13/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9991\n",
      "Epoch 00013: val_loss did not improve from 4.09126\n",
      "32/32 [==============================] - 9s 288ms/step - loss: 3.9991 - val_loss: 10.1817 - lr: 5.0000e-04\n",
      "Epoch 14/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.7327\n",
      "Epoch 00014: val_loss did not improve from 4.09126\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 5.7327 - val_loss: 26.2518 - lr: 5.0000e-04\n",
      "Epoch 15/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5282\n",
      "Epoch 00015: val_loss did not improve from 4.09126\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 4.5282 - val_loss: 32.3773 - lr: 5.0000e-04\n",
      "Epoch 16/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9643\n",
      "Epoch 00016: val_loss did not improve from 4.09126\n",
      "32/32 [==============================] - 9s 287ms/step - loss: 4.9643 - val_loss: 17.5319 - lr: 2.5000e-04\n",
      "Epoch 17/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.1204\n",
      "Epoch 00017: val_loss did not improve from 4.09126\n",
      "32/32 [==============================] - 9s 289ms/step - loss: 5.1204 - val_loss: 14.2626 - lr: 2.5000e-04\n",
      "Epoch 18/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1080\n",
      "Epoch 00018: val_loss did not improve from 4.09126\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 4.1080 - val_loss: 6.4708 - lr: 2.5000e-04\n",
      "Epoch 19/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4255\n",
      "Epoch 00019: val_loss did not improve from 4.09126\n",
      "32/32 [==============================] - 9s 296ms/step - loss: 4.4255 - val_loss: 7.2398 - lr: 2.5000e-04\n",
      "Epoch 20/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2035\n",
      "Epoch 00020: val_loss did not improve from 4.09126\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "32/32 [==============================] - 9s 296ms/step - loss: 4.2035 - val_loss: 10.8760 - lr: 2.5000e-04\n",
      "Epoch 21/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0463\n",
      "Epoch 00021: val_loss did not improve from 4.09126\n",
      "32/32 [==============================] - 9s 293ms/step - loss: 4.0463 - val_loss: 8.2844 - lr: 1.2500e-04\n",
      "Epoch 22/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2544\n",
      "Epoch 00022: val_loss did not improve from 4.09126\n",
      "32/32 [==============================] - 9s 292ms/step - loss: 4.2544 - val_loss: 11.8858 - lr: 1.2500e-04\n",
      "Epoch 23/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.2335\n",
      "Epoch 00023: val_loss improved from 4.09126 to 3.70102, saving model to C:/Users/Monir/Documents/CSE499/models/EfficientNet/b7_80_epochs.h5\n",
      "32/32 [==============================] - 25s 770ms/step - loss: 5.2335 - val_loss: 3.7010 - lr: 1.2500e-04\n",
      "Epoch 24/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9875\n",
      "Epoch 00024: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 10s 309ms/step - loss: 3.9875 - val_loss: 5.8713 - lr: 1.2500e-04\n",
      "Epoch 25/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7448\n",
      "Epoch 00025: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 289ms/step - loss: 3.7448 - val_loss: 8.5656 - lr: 1.2500e-04\n",
      "Epoch 26/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8185\n",
      "Epoch 00026: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 289ms/step - loss: 3.8185 - val_loss: 5.9540 - lr: 1.2500e-04\n",
      "Epoch 27/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 2.7490\n",
      "Epoch 00027: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 287ms/step - loss: 2.7490 - val_loss: 7.6990 - lr: 1.2500e-04\n",
      "Epoch 28/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.3728\n",
      "Epoch 00028: val_loss did not improve from 3.70102\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "32/32 [==============================] - 9s 289ms/step - loss: 3.3728 - val_loss: 5.8719 - lr: 1.2500e-04\n",
      "Epoch 29/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2994\n",
      "Epoch 00029: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 285ms/step - loss: 4.2994 - val_loss: 9.5323 - lr: 6.2500e-05\n",
      "Epoch 30/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.8131\n",
      "Epoch 00030: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 287ms/step - loss: 5.8131 - val_loss: 13.2560 - lr: 6.2500e-05\n",
      "Epoch 31/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6643\n",
      "Epoch 00031: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 287ms/step - loss: 3.6643 - val_loss: 12.3122 - lr: 6.2500e-05\n",
      "Epoch 32/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4920\n",
      "Epoch 00032: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 290ms/step - loss: 4.4920 - val_loss: 6.1149 - lr: 6.2500e-05\n",
      "Epoch 33/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4941\n",
      "Epoch 00033: val_loss did not improve from 3.70102\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "32/32 [==============================] - 9s 288ms/step - loss: 4.4941 - val_loss: 3.9614 - lr: 6.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5672\n",
      "Epoch 00034: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 288ms/step - loss: 4.5672 - val_loss: 5.8717 - lr: 3.1250e-05\n",
      "Epoch 35/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9734\n",
      "Epoch 00035: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 289ms/step - loss: 4.9734 - val_loss: 10.8709 - lr: 3.1250e-05\n",
      "Epoch 36/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0126\n",
      "Epoch 00036: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 289ms/step - loss: 4.0126 - val_loss: 8.8674 - lr: 3.1250e-05\n",
      "Epoch 37/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.4485\n",
      "Epoch 00037: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 291ms/step - loss: 3.4485 - val_loss: 12.5263 - lr: 3.1250e-05\n",
      "Epoch 38/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.0415\n",
      "Epoch 00038: val_loss did not improve from 3.70102\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "32/32 [==============================] - 9s 287ms/step - loss: 5.0415 - val_loss: 7.9011 - lr: 3.1250e-05\n",
      "Epoch 39/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1093\n",
      "Epoch 00039: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 288ms/step - loss: 4.1093 - val_loss: 4.8915 - lr: 1.5625e-05\n",
      "Epoch 40/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.0136\n",
      "Epoch 00040: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 288ms/step - loss: 5.0136 - val_loss: 8.4763 - lr: 1.5625e-05\n",
      "Epoch 41/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6471\n",
      "Epoch 00041: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 4.6471 - val_loss: 9.0391 - lr: 1.5625e-05\n",
      "Epoch 42/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2602\n",
      "Epoch 00042: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 289ms/step - loss: 4.2602 - val_loss: 8.8847 - lr: 1.5625e-05\n",
      "Epoch 43/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7539\n",
      "Epoch 00043: val_loss did not improve from 3.70102\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 4.7539 - val_loss: 12.1074 - lr: 1.5625e-05\n",
      "Epoch 44/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.1864\n",
      "Epoch 00044: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 291ms/step - loss: 5.1864 - val_loss: 6.5920 - lr: 7.8125e-06\n",
      "Epoch 45/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2413\n",
      "Epoch 00045: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 287ms/step - loss: 4.2413 - val_loss: 5.5250 - lr: 7.8125e-06\n",
      "Epoch 46/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6545\n",
      "Epoch 00046: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 3.6545 - val_loss: 11.7464 - lr: 7.8125e-06\n",
      "Epoch 47/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.3230\n",
      "Epoch 00047: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 287ms/step - loss: 3.3230 - val_loss: 5.6578 - lr: 7.8125e-06\n",
      "Epoch 48/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.4429\n",
      "Epoch 00048: val_loss did not improve from 3.70102\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "32/32 [==============================] - 9s 285ms/step - loss: 5.4429 - val_loss: 6.7638 - lr: 7.8125e-06\n",
      "Epoch 49/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1117\n",
      "Epoch 00049: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 289ms/step - loss: 4.1117 - val_loss: 7.2432 - lr: 3.9063e-06\n",
      "Epoch 50/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2405\n",
      "Epoch 00050: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 285ms/step - loss: 4.2405 - val_loss: 9.1707 - lr: 3.9063e-06\n",
      "Epoch 51/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7603\n",
      "Epoch 00051: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 292ms/step - loss: 4.7603 - val_loss: 16.3237 - lr: 3.9063e-06\n",
      "Epoch 52/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.4559\n",
      "Epoch 00052: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 291ms/step - loss: 5.4559 - val_loss: 9.1395 - lr: 3.9063e-06\n",
      "Epoch 53/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.4618\n",
      "Epoch 00053: val_loss did not improve from 3.70102\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "32/32 [==============================] - 9s 290ms/step - loss: 3.4618 - val_loss: 10.7358 - lr: 3.9063e-06\n",
      "Epoch 54/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8000\n",
      "Epoch 00054: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 290ms/step - loss: 4.8000 - val_loss: 9.0097 - lr: 1.9531e-06\n",
      "Epoch 55/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1143\n",
      "Epoch 00055: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 290ms/step - loss: 4.1143 - val_loss: 12.5108 - lr: 1.9531e-06\n",
      "Epoch 56/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 2.6658\n",
      "Epoch 00056: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 287ms/step - loss: 2.6658 - val_loss: 6.0538 - lr: 1.9531e-06\n",
      "Epoch 57/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4590\n",
      "Epoch 00057: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 4.4590 - val_loss: 6.1759 - lr: 1.9531e-06\n",
      "Epoch 58/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8851\n",
      "Epoch 00058: val_loss did not improve from 3.70102\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "32/32 [==============================] - 9s 284ms/step - loss: 3.8851 - val_loss: 7.9223 - lr: 1.9531e-06\n",
      "Epoch 59/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1096\n",
      "Epoch 00059: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 4.1096 - val_loss: 9.3143 - lr: 9.7656e-07\n",
      "Epoch 60/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6766\n",
      "Epoch 00060: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 285ms/step - loss: 3.6766 - val_loss: 9.2207 - lr: 9.7656e-07\n",
      "Epoch 61/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3125\n",
      "Epoch 00061: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 4.3125 - val_loss: 6.3485 - lr: 9.7656e-07\n",
      "Epoch 62/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9501\n",
      "Epoch 00062: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 285ms/step - loss: 3.9501 - val_loss: 4.4143 - lr: 9.7656e-07\n",
      "Epoch 63/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6077\n",
      "Epoch 00063: val_loss did not improve from 3.70102\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "32/32 [==============================] - 9s 289ms/step - loss: 4.6077 - val_loss: 9.3490 - lr: 9.7656e-07\n",
      "Epoch 64/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8848\n",
      "Epoch 00064: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 4.8848 - val_loss: 8.6722 - lr: 4.8828e-07\n",
      "Epoch 65/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7092\n",
      "Epoch 00065: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 290ms/step - loss: 3.7092 - val_loss: 8.9082 - lr: 4.8828e-07\n",
      "Epoch 66/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 2.7755\n",
      "Epoch 00066: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 291ms/step - loss: 2.7755 - val_loss: 12.5412 - lr: 4.8828e-07\n",
      "Epoch 67/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - ETA: 0s - loss: 4.9850\n",
      "Epoch 00067: val_loss did not improve from 3.70102\n",
      "32/32 [==============================] - 9s 285ms/step - loss: 4.9850 - val_loss: 6.5127 - lr: 4.8828e-07\n",
      "Epoch 68/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3231\n",
      "Epoch 00068: val_loss improved from 3.70102 to 3.45796, saving model to C:/Users/Monir/Documents/CSE499/models/EfficientNet/b7_80_epochs.h5\n",
      "32/32 [==============================] - 24s 759ms/step - loss: 4.3231 - val_loss: 3.4580 - lr: 4.8828e-07\n",
      "Epoch 69/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9137\n",
      "Epoch 00069: val_loss did not improve from 3.45796\n",
      "32/32 [==============================] - 10s 301ms/step - loss: 4.9137 - val_loss: 7.4916 - lr: 4.8828e-07\n",
      "Epoch 70/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9233\n",
      "Epoch 00070: val_loss did not improve from 3.45796\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 3.9233 - val_loss: 7.4812 - lr: 4.8828e-07\n",
      "Epoch 71/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.4950\n",
      "Epoch 00071: val_loss did not improve from 3.45796\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 5.4950 - val_loss: 10.4477 - lr: 4.8828e-07\n",
      "Epoch 72/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9466\n",
      "Epoch 00072: val_loss did not improve from 3.45796\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 3.9466 - val_loss: 5.4820 - lr: 4.8828e-07\n",
      "Epoch 73/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2264\n",
      "Epoch 00073: val_loss did not improve from 3.45796\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "32/32 [==============================] - 9s 284ms/step - loss: 4.2264 - val_loss: 7.9376 - lr: 4.8828e-07\n",
      "Epoch 74/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9660\n",
      "Epoch 00074: val_loss did not improve from 3.45796\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 3.9660 - val_loss: 9.2762 - lr: 2.4414e-07\n",
      "Epoch 75/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2879\n",
      "Epoch 00075: val_loss did not improve from 3.45796\n",
      "32/32 [==============================] - 9s 285ms/step - loss: 4.2879 - val_loss: 5.9371 - lr: 2.4414e-07\n",
      "Epoch 76/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.8726\n",
      "Epoch 00076: val_loss did not improve from 3.45796\n",
      "32/32 [==============================] - 9s 286ms/step - loss: 5.8726 - val_loss: 8.7194 - lr: 2.4414e-07\n",
      "Epoch 77/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1805\n",
      "Epoch 00077: val_loss did not improve from 3.45796\n",
      "32/32 [==============================] - 9s 284ms/step - loss: 4.1805 - val_loss: 8.7038 - lr: 2.4414e-07\n",
      "Epoch 78/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6331\n",
      "Epoch 00078: val_loss did not improve from 3.45796\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "32/32 [==============================] - 9s 285ms/step - loss: 3.6331 - val_loss: 6.4217 - lr: 2.4414e-07\n",
      "Epoch 79/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6750\n",
      "Epoch 00079: val_loss did not improve from 3.45796\n",
      "32/32 [==============================] - 9s 285ms/step - loss: 3.6750 - val_loss: 5.8064 - lr: 1.2207e-07\n",
      "Epoch 80/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7104\n",
      "Epoch 00080: val_loss did not improve from 3.45796\n",
      "32/32 [==============================] - 9s 287ms/step - loss: 4.7104 - val_loss: 7.0828 - lr: 1.2207e-07\n",
      "Training Complete!!!\n"
     ]
    }
   ],
   "source": [
    "P = np.array(P)\n",
    "subs = []\n",
    "folds_history = []\n",
    "\n",
    "\"\"\"\n",
    "er = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=1e-3,patience=15,verbose=1,mode=\"auto\",baseline=None,\n",
    "      restore_best_weights=True,) #Stop training when a monitored metric has stopped improving.\n",
    "\"\"\"\n",
    "\n",
    "cpt = tf.keras.callbacks.ModelCheckpoint(filepath=f'C:/Users/Monir/Documents/CSE499/models/EfficientNet/{MODEL_CLASS}_{EPOCHS}_epochs.h5',monitor='val_loss',verbose=1, \n",
    "    save_best_only=SAVE_BEST,mode='auto') #to save model or weights in a checkpoint file at lowest validation loss\n",
    "\n",
    "rlp = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=5, verbose=1, min_lr=1e-8) \n",
    "     #Reduce learning rate when a metric has stopped improving.\n",
    "     # if improvement stops, after 5 epochs learning rate will be reduced\n",
    "\n",
    "model = build_model(model_class=MODEL_CLASS)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR), loss=\"mae\") \n",
    "history = model.fit(IGenerator(keys=P, a = A, tab = TAB, batch_size=BATCH_SIZE), \n",
    "                    steps_per_epoch = 32,\n",
    "                    validation_data=IGenerator(keys=P, a = A, tab = TAB, batch_size=BATCH_SIZE),\n",
    "                    validation_steps = 16, \n",
    "                    callbacks = [cpt, rlp], \n",
    "                    epochs=EPOCHS)\n",
    "folds_history.append(history.history)\n",
    "print('Training Complete!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "# save to json:  \n",
    "hist_json_file = 'EffNet_b7_80_epoch_history.json' \n",
    "with open(hist_json_file, mode='w') as f:\n",
    "    hist_df.to_json(f)\n",
    "\n",
    "# or save to csv: \n",
    "hist_csv_file = 'EffNet_b7_80_epoch_history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = 'C:/Users/Monir/Documents/CSE499/results_and_figures/EfficientNet/B7/'\n",
    "\n",
    "import tikzplotlib\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(EPOCHS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs_range, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.savefig(result_dir + 'b7_80_epoch_oss.png')\n",
    "plt.savefig(result_dir + 'b7_80_epoch_loss.pdf', dpi=150)\n",
    "tikzplotlib.save(result_dir + 'b7_80_epoch_loss.tex')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_range = range(15,EPOCHS)\n",
    "\n",
    "plt.plot(epochs_range,\n",
    "         smooth_curve(loss[15:]), 'bo', label='Training loss')\n",
    "plt.plot(epochs_range,\n",
    "         smooth_curve(val_loss[15:]), 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig(result_dir + 'b7_80_epoch_loss_smooth.png')\n",
    "plt.savefig(result_dir + 'b7_80_epoch_smooth.pdf', dpi=150)\n",
    "tikzplotlib.save(result_dir + 'b7_80_epoch_smooth.tex')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
