{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('D:/CSE499/osic-pulmonary-fibrosis-progression/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>Weeks</th>\n",
       "      <th>FVC</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SmokingStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>-4</td>\n",
       "      <td>2315</td>\n",
       "      <td>58.253649</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>5</td>\n",
       "      <td>2214</td>\n",
       "      <td>55.712129</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>7</td>\n",
       "      <td>2061</td>\n",
       "      <td>51.862104</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>9</td>\n",
       "      <td>2144</td>\n",
       "      <td>53.950679</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>11</td>\n",
       "      <td>2069</td>\n",
       "      <td>52.063412</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Patient  Weeks   FVC    Percent  Age   Sex SmokingStatus\n",
       "0  ID00007637202177411956430     -4  2315  58.253649   79  Male     Ex-smoker\n",
       "1  ID00007637202177411956430      5  2214  55.712129   79  Male     Ex-smoker\n",
       "2  ID00007637202177411956430      7  2061  51.862104   79  Male     Ex-smoker\n",
       "3  ID00007637202177411956430      9  2144  53.950679   79  Male     Ex-smoker\n",
       "4  ID00007637202177411956430     11  2069  52.063412   79  Male     Ex-smoker"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ex-smoker', 'Never smoked', 'Currently smokes'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.SmokingStatus.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tab(df):\n",
    "    \"raturn an array which contains each patient normalized age, sex and smoking status\"\n",
    "    vector = [(df.Age.values[0] - 30) / 30] \n",
    "    \n",
    "    if df.Sex.values[0].lower() == 'male':\n",
    "       vector.append(0)\n",
    "    else:\n",
    "       vector.append(1)\n",
    "    \n",
    "    if df.SmokingStatus.values[0] == 'Never smoked':\n",
    "        vector.extend([0,0])\n",
    "    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n",
    "        vector.extend([1,1])\n",
    "    elif df.SmokingStatus.values[0] == 'Currently smokes':\n",
    "        vector.extend([0,1])\n",
    "    else:\n",
    "        vector.extend([1,0])\n",
    "    return np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b8f27b62b44888956cce12355fc08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Monir\\anaconda3\\envs\\rabbi36\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "A = {} \n",
    "TAB = {} \n",
    "P = [] \n",
    "for i, p in tqdm(enumerate(train.Patient.unique())): # i index, p patient id\n",
    "    sub = train.loc[train.Patient == p, :] # find all data (weeks, FVC, Percent, Age, Sex, SmokingStatus) of a unique patient\n",
    "    fvc = sub.FVC.values # fvc values of the patient during the follow-up\n",
    "    weeks = sub.Weeks.values # follow-up weeks\n",
    "    c = np.vstack([weeks, np.ones(len(weeks))]).T # create an array by the follow-up weeks of shape(len(weeks),2)\n",
    "    a, b = np.linalg.lstsq(c, fvc)[0] # least-square sol, a=gradient matrix, b=right hand matrix \n",
    "    \n",
    "    A[p] = a\n",
    "    TAB[p] = get_tab(sub)\n",
    "    P.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(path):\n",
    "    \"read DICOM dataset and return resize images of size (512,512,1)\"\n",
    "    d = pydicom.dcmread(path) # read and parse the CT scan images (in DICOM format)\n",
    "    resized_image = cv2.resize((d.pixel_array - d.RescaleIntercept) / (d.RescaleSlope * 1000), (512,512))\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class IGenerator(Sequence):\n",
    "    BAD_ID = ['ID00011637202177653955184', 'ID00052637202186188008618']\n",
    "    def __init__(self, keys, a, tab, batch_size):\n",
    "        \"key=patient, a=gradient matrix, tab=a particular patient's data\"\n",
    "        self.keys = [k for k in keys if k not in self.BAD_ID]\n",
    "        self.a = a\n",
    "        self.tab = tab\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.train_data = {}\n",
    "        for p in train.Patient.values:\n",
    "            self.train_data[p] = os.listdir(f'D:/CSE499/osic-pulmonary-fibrosis-progression/train/{p}/')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1000\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = []\n",
    "        a, tab = [], [] \n",
    "        keys = np.random.choice(self.keys, size = self.batch_size) # randomly chooses n=batch_size number of patients\n",
    "        for k in keys:\n",
    "            try:\n",
    "                i = np.random.choice(self.train_data[k], size=1)[0] # chooses some randomly images for kth patient\n",
    "                img = get_img(f'D:/CSE499/osic-pulmonary-fibrosis-progression/train/{k}/{i}') # resizes ith image of kth patient\n",
    "                x.append(img) # append kth patient's image data in the list x\n",
    "                a.append(self.a[k]) # append kth patient's gradinet in the list a\n",
    "                tab.append(self.tab[k]) # append kth patient's tabular data in the tab list\n",
    "            except:\n",
    "                print(k, i)\n",
    "       \n",
    "        x,a,tab = np.array(x), np.array(a), np.array(tab) # convert list to array\n",
    "        x = np.expand_dims(x, axis=-1) \n",
    "        return [x, tab] , a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, Activation, Flatten, Input, BatchNormalization, GlobalAveragePooling2D, GaussianNoise, \n",
    "    Add, Conv2D, AveragePooling2D, LeakyReLU, Concatenate \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "def get_efficientnet(model, shape):\n",
    "    models_dict = {\n",
    "        'b0': efn.EfficientNetB0(input_shape=shape,weights=None,include_top=False),\n",
    "        'b1': efn.EfficientNetB1(input_shape=shape,weights=None,include_top=False),\n",
    "        'b2': efn.EfficientNetB2(input_shape=shape,weights=None,include_top=False),\n",
    "        'b3': efn.EfficientNetB3(input_shape=shape,weights=None,include_top=False),\n",
    "        'b4': efn.EfficientNetB4(input_shape=shape,weights=None,include_top=False),\n",
    "        'b5': efn.EfficientNetB5(input_shape=shape,weights=None,include_top=False),\n",
    "        'b6': efn.EfficientNetB6(input_shape=shape,weights=None,include_top=False),\n",
    "        'b7': efn.EfficientNetB7(input_shape=shape,weights=None,include_top=False)\n",
    "    }\n",
    "    return models_dict[model]\n",
    "\n",
    "def build_model(shape=(512, 512, 1), model_class=None):\n",
    "    inp = Input(shape=shape)\n",
    "    base = get_efficientnet(model_class, shape)\n",
    "    x = base(inp)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    inp2 = Input(shape=(4,)) # indicates that the expected input will be batches of 4-dimensional vectors\n",
    "    x2 = GaussianNoise(0.2)(inp2) # to mitigate overfitting\n",
    "    x = Concatenate()([x, x2]) \n",
    "    x = Dropout(0.5)(x) \n",
    "    x = Dense(1)(x)\n",
    "    model = Model([inp, inp2] , x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512, 512, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "efficientnet-b1 (Model)         (None, 16, 16, 1280) 6574656     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1280)         0           efficientnet-b1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 4)            0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1284)         0           global_average_pooling2d[0][0]   \n",
      "                                                                 gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1284)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1285        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,575,941\n",
      "Trainable params: 6,513,893\n",
      "Non-trainable params: 62,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MODEL_CLASS = 'b1'\n",
    "base_model = build_model(shape=(512, 512, 1), model_class=MODEL_CLASS)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "EPOCHS = 80\n",
    "LR = 0.001\n",
    "SAVE_BEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7453\n",
      "Epoch 00001: val_loss improved from inf to 4.65520, saving model to C:/Users/Monir/Documents/CSE499/models/EfficientNet/b1_80_epochs.h5\n",
      "32/32 [==============================] - 11s 359ms/step - loss: 4.7453 - val_loss: 4.6552 - lr: 0.0010\n",
      "Epoch 2/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.2834\n",
      "Epoch 00002: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 6s 193ms/step - loss: 5.2834 - val_loss: 5.6069 - lr: 0.0010\n",
      "Epoch 3/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2183\n",
      "Epoch 00003: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 6s 177ms/step - loss: 4.2183 - val_loss: 7.7607 - lr: 0.0010\n",
      "Epoch 4/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9402\n",
      "Epoch 00004: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 6s 173ms/step - loss: 4.9402 - val_loss: 7.8061 - lr: 0.0010\n",
      "Epoch 5/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.5154\n",
      "Epoch 00005: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 5.5154 - val_loss: 5.9491 - lr: 0.0010\n",
      "Epoch 6/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7995\n",
      "Epoch 00006: val_loss did not improve from 4.65520\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 4.7995 - val_loss: 91.0134 - lr: 0.0010\n",
      "Epoch 7/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.5444\n",
      "Epoch 00007: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 6s 179ms/step - loss: 5.5444 - val_loss: 378.1748 - lr: 5.0000e-04\n",
      "Epoch 8/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.7055\n",
      "Epoch 00008: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 5.7055 - val_loss: 173.0728 - lr: 5.0000e-04\n",
      "Epoch 9/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.0847\n",
      "Epoch 00009: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 5.0847 - val_loss: 74.1255 - lr: 5.0000e-04\n",
      "Epoch 10/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3400\n",
      "Epoch 00010: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 4.3400 - val_loss: 51.4453 - lr: 5.0000e-04\n",
      "Epoch 11/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7333\n",
      "Epoch 00011: val_loss did not improve from 4.65520\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 4.7333 - val_loss: 48.8108 - lr: 5.0000e-04\n",
      "Epoch 12/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9138\n",
      "Epoch 00012: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 4.9138 - val_loss: 16.1136 - lr: 2.5000e-04\n",
      "Epoch 13/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6065\n",
      "Epoch 00013: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 4.6065 - val_loss: 30.8931 - lr: 2.5000e-04\n",
      "Epoch 14/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1525\n",
      "Epoch 00014: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 6s 174ms/step - loss: 4.1525 - val_loss: 33.5823 - lr: 2.5000e-04\n",
      "Epoch 15/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1830\n",
      "Epoch 00015: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 4.1830 - val_loss: 25.9155 - lr: 2.5000e-04\n",
      "Epoch 16/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9107\n",
      "Epoch 00016: val_loss did not improve from 4.65520\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 3.9107 - val_loss: 25.8461 - lr: 2.5000e-04\n",
      "Epoch 17/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.0825\n",
      "Epoch 00017: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 3.0825 - val_loss: 27.4815 - lr: 1.2500e-04\n",
      "Epoch 18/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3363\n",
      "Epoch 00018: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 4.3363 - val_loss: 26.1119 - lr: 1.2500e-04\n",
      "Epoch 19/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7058\n",
      "Epoch 00019: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 4.7058 - val_loss: 23.0770 - lr: 1.2500e-04\n",
      "Epoch 20/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9882\n",
      "Epoch 00020: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 3.9882 - val_loss: 21.3997 - lr: 1.2500e-04\n",
      "Epoch 21/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3406\n",
      "Epoch 00021: val_loss did not improve from 4.65520\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "32/32 [==============================] - 5s 163ms/step - loss: 4.3406 - val_loss: 16.3433 - lr: 1.2500e-04\n",
      "Epoch 22/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9189\n",
      "Epoch 00022: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 3.9189 - val_loss: 16.6860 - lr: 6.2500e-05\n",
      "Epoch 23/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8107\n",
      "Epoch 00023: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 4.8107 - val_loss: 15.6926 - lr: 6.2500e-05\n",
      "Epoch 24/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9621\n",
      "Epoch 00024: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 162ms/step - loss: 3.9621 - val_loss: 17.0684 - lr: 6.2500e-05\n",
      "Epoch 25/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4486\n",
      "Epoch 00025: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 162ms/step - loss: 4.4486 - val_loss: 16.5492 - lr: 6.2500e-05\n",
      "Epoch 26/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3225\n",
      "Epoch 00026: val_loss did not improve from 4.65520\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "32/32 [==============================] - 5s 163ms/step - loss: 4.3225 - val_loss: 17.5324 - lr: 6.2500e-05\n",
      "Epoch 27/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4110\n",
      "Epoch 00027: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 4.4110 - val_loss: 14.6725 - lr: 3.1250e-05\n",
      "Epoch 28/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.1917\n",
      "Epoch 00028: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 164ms/step - loss: 5.1917 - val_loss: 11.6126 - lr: 3.1250e-05\n",
      "Epoch 29/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6881\n",
      "Epoch 00029: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 164ms/step - loss: 4.6881 - val_loss: 16.2355 - lr: 3.1250e-05\n",
      "Epoch 30/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.4061\n",
      "Epoch 00030: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 3.4061 - val_loss: 17.1779 - lr: 3.1250e-05\n",
      "Epoch 31/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5329\n",
      "Epoch 00031: val_loss did not improve from 4.65520\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "32/32 [==============================] - 5s 161ms/step - loss: 4.5329 - val_loss: 17.7896 - lr: 3.1250e-05\n",
      "Epoch 32/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8197\n",
      "Epoch 00032: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 165ms/step - loss: 3.8197 - val_loss: 18.6159 - lr: 1.5625e-05\n",
      "Epoch 33/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2302\n",
      "Epoch 00033: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 162ms/step - loss: 4.2302 - val_loss: 14.8522 - lr: 1.5625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6592\n",
      "Epoch 00034: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 162ms/step - loss: 3.6592 - val_loss: 15.8397 - lr: 1.5625e-05\n",
      "Epoch 35/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.3269\n",
      "Epoch 00035: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 3.3269 - val_loss: 12.7560 - lr: 1.5625e-05\n",
      "Epoch 36/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9839\n",
      "Epoch 00036: val_loss did not improve from 4.65520\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "32/32 [==============================] - 5s 165ms/step - loss: 3.9839 - val_loss: 14.0430 - lr: 1.5625e-05\n",
      "Epoch 37/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3135\n",
      "Epoch 00037: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 158ms/step - loss: 4.3135 - val_loss: 14.6091 - lr: 7.8125e-06\n",
      "Epoch 38/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5840\n",
      "Epoch 00038: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 164ms/step - loss: 4.5840 - val_loss: 14.8137 - lr: 7.8125e-06\n",
      "Epoch 39/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6875\n",
      "Epoch 00039: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 3.6875 - val_loss: 13.9640 - lr: 7.8125e-06\n",
      "Epoch 40/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2398\n",
      "Epoch 00040: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 165ms/step - loss: 4.2398 - val_loss: 15.6696 - lr: 7.8125e-06\n",
      "Epoch 41/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2748\n",
      "Epoch 00041: val_loss did not improve from 4.65520\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 4.2748 - val_loss: 14.1760 - lr: 7.8125e-06\n",
      "Epoch 42/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6168\n",
      "Epoch 00042: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 162ms/step - loss: 3.6168 - val_loss: 13.4848 - lr: 3.9063e-06\n",
      "Epoch 43/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9805\n",
      "Epoch 00043: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 3.9805 - val_loss: 15.1972 - lr: 3.9063e-06\n",
      "Epoch 44/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0580\n",
      "Epoch 00044: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 162ms/step - loss: 4.0580 - val_loss: 12.2604 - lr: 3.9063e-06\n",
      "Epoch 45/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2931\n",
      "Epoch 00045: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 162ms/step - loss: 4.2931 - val_loss: 13.9648 - lr: 3.9063e-06\n",
      "Epoch 46/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9953\n",
      "Epoch 00046: val_loss did not improve from 4.65520\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "32/32 [==============================] - 5s 160ms/step - loss: 3.9953 - val_loss: 14.1268 - lr: 3.9063e-06\n",
      "Epoch 47/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2902\n",
      "Epoch 00047: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 163ms/step - loss: 4.2902 - val_loss: 11.1089 - lr: 1.9531e-06\n",
      "Epoch 48/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6794\n",
      "Epoch 00048: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 161ms/step - loss: 4.6794 - val_loss: 14.0144 - lr: 1.9531e-06\n",
      "Epoch 49/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9994\n",
      "Epoch 00049: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 159ms/step - loss: 4.9994 - val_loss: 13.0289 - lr: 1.9531e-06\n",
      "Epoch 50/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7030\n",
      "Epoch 00050: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 158ms/step - loss: 3.7030 - val_loss: 14.1458 - lr: 1.9531e-06\n",
      "Epoch 51/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0570\n",
      "Epoch 00051: val_loss did not improve from 4.65520\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "32/32 [==============================] - 5s 161ms/step - loss: 4.0570 - val_loss: 14.9193 - lr: 1.9531e-06\n",
      "Epoch 52/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3692\n",
      "Epoch 00052: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 165ms/step - loss: 4.3692 - val_loss: 14.9463 - lr: 9.7656e-07\n",
      "Epoch 53/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3165\n",
      "Epoch 00053: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 157ms/step - loss: 4.3165 - val_loss: 17.3493 - lr: 9.7656e-07\n",
      "Epoch 54/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1876\n",
      "Epoch 00054: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 161ms/step - loss: 4.1876 - val_loss: 16.5855 - lr: 9.7656e-07\n",
      "Epoch 55/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6856\n",
      "Epoch 00055: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 161ms/step - loss: 4.6856 - val_loss: 14.0258 - lr: 9.7656e-07\n",
      "Epoch 56/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2443\n",
      "Epoch 00056: val_loss did not improve from 4.65520\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "32/32 [==============================] - 5s 163ms/step - loss: 4.2443 - val_loss: 16.2513 - lr: 9.7656e-07\n",
      "Epoch 57/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.2219\n",
      "Epoch 00057: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 157ms/step - loss: 3.2219 - val_loss: 15.3074 - lr: 4.8828e-07\n",
      "Epoch 58/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0673\n",
      "Epoch 00058: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 4.0673 - val_loss: 16.4312 - lr: 4.8828e-07\n",
      "Epoch 59/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0426\n",
      "Epoch 00059: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 156ms/step - loss: 4.0426 - val_loss: 16.5849 - lr: 4.8828e-07\n",
      "Epoch 60/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.4044\n",
      "Epoch 00060: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 3.4044 - val_loss: 16.8052 - lr: 4.8828e-07\n",
      "Epoch 61/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7699\n",
      "Epoch 00061: val_loss did not improve from 4.65520\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "32/32 [==============================] - 5s 156ms/step - loss: 4.7699 - val_loss: 16.6507 - lr: 4.8828e-07\n",
      "Epoch 62/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2582\n",
      "Epoch 00062: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 159ms/step - loss: 4.2582 - val_loss: 16.3170 - lr: 2.4414e-07\n",
      "Epoch 63/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0388\n",
      "Epoch 00063: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 157ms/step - loss: 4.0388 - val_loss: 18.8670 - lr: 2.4414e-07\n",
      "Epoch 64/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8661\n",
      "Epoch 00064: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 161ms/step - loss: 3.8661 - val_loss: 16.7940 - lr: 2.4414e-07\n",
      "Epoch 65/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8959\n",
      "Epoch 00065: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 161ms/step - loss: 3.8959 - val_loss: 17.5447 - lr: 2.4414e-07\n",
      "Epoch 66/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6034\n",
      "Epoch 00066: val_loss did not improve from 4.65520\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "32/32 [==============================] - 5s 159ms/step - loss: 3.6034 - val_loss: 16.3087 - lr: 2.4414e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1650\n",
      "Epoch 00067: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 4.1650 - val_loss: 12.9403 - lr: 1.2207e-07\n",
      "Epoch 68/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4895\n",
      "Epoch 00068: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 157ms/step - loss: 4.4895 - val_loss: 13.3784 - lr: 1.2207e-07\n",
      "Epoch 69/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0347\n",
      "Epoch 00069: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 158ms/step - loss: 4.0347 - val_loss: 16.3193 - lr: 1.2207e-07\n",
      "Epoch 70/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5500\n",
      "Epoch 00070: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 158ms/step - loss: 4.5500 - val_loss: 14.3284 - lr: 1.2207e-07\n",
      "Epoch 71/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7750\n",
      "Epoch 00071: val_loss did not improve from 4.65520\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "32/32 [==============================] - 5s 160ms/step - loss: 4.7750 - val_loss: 15.8410 - lr: 1.2207e-07\n",
      "Epoch 72/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.2939\n",
      "Epoch 00072: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 157ms/step - loss: 5.2939 - val_loss: 15.7564 - lr: 6.1035e-08\n",
      "Epoch 73/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0470\n",
      "Epoch 00073: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 157ms/step - loss: 4.0470 - val_loss: 12.6695 - lr: 6.1035e-08\n",
      "Epoch 74/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.1804\n",
      "Epoch 00074: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 158ms/step - loss: 5.1804 - val_loss: 14.5118 - lr: 6.1035e-08\n",
      "Epoch 75/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3787\n",
      "Epoch 00075: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 156ms/step - loss: 4.3787 - val_loss: 13.0531 - lr: 6.1035e-08\n",
      "Epoch 76/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2495\n",
      "Epoch 00076: val_loss did not improve from 4.65520\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "32/32 [==============================] - 5s 157ms/step - loss: 4.2495 - val_loss: 16.0930 - lr: 6.1035e-08\n",
      "Epoch 77/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7971\n",
      "Epoch 00077: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 159ms/step - loss: 3.7971 - val_loss: 13.5964 - lr: 3.0518e-08\n",
      "Epoch 78/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.2666\n",
      "Epoch 00078: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 3.2666 - val_loss: 16.4101 - lr: 3.0518e-08\n",
      "Epoch 79/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9607\n",
      "Epoch 00079: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 161ms/step - loss: 4.9607 - val_loss: 13.4518 - lr: 3.0518e-08\n",
      "Epoch 80/80\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2291\n",
      "Epoch 00080: val_loss did not improve from 4.65520\n",
      "32/32 [==============================] - 5s 159ms/step - loss: 4.2291 - val_loss: 18.9222 - lr: 3.0518e-08\n",
      "Training Complete!!!\n"
     ]
    }
   ],
   "source": [
    "P = np.array(P)\n",
    "subs = []\n",
    "folds_history = []\n",
    "\n",
    "\"\"\"\n",
    "er = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=1e-3,patience=15,verbose=1,mode=\"auto\",baseline=None,\n",
    "      restore_best_weights=True,) #Stop training when a monitored metric has stopped improving.\n",
    "\"\"\"\n",
    "\n",
    "cpt = tf.keras.callbacks.ModelCheckpoint(filepath=f'C:/Users/Monir/Documents/CSE499/models/EfficientNet/{MODEL_CLASS}_{EPOCHS}_epochs.h5',monitor='val_loss',verbose=1, \n",
    "    save_best_only=SAVE_BEST,mode='auto') #to save model or weights in a checkpoint file at lowest validation loss\n",
    "\n",
    "rlp = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=5, verbose=1, min_lr=1e-8) \n",
    "     #Reduce learning rate when a metric has stopped improving.\n",
    "     # if improvement stops, after 5 epochs learning rate will be reduced\n",
    "\n",
    "model = build_model(model_class=MODEL_CLASS)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR), loss=\"mae\") \n",
    "history = model.fit(IGenerator(keys=P, a = A, tab = TAB, batch_size=BATCH_SIZE), \n",
    "                    steps_per_epoch = 32,\n",
    "                    validation_data=IGenerator(keys=P, a = A, tab = TAB, batch_size=BATCH_SIZE),\n",
    "                    validation_steps = 32, \n",
    "                    callbacks = [cpt, rlp], \n",
    "                    epochs=EPOCHS)\n",
    "folds_history.append(history.history)\n",
    "print('Training Complete!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "\n",
    "# save to json:  \n",
    "hist_json_file = 'EffNet_b1_80_epoch_history.json' \n",
    "with open(hist_json_file, mode='w') as f:\n",
    "    hist_df.to_json(f)\n",
    "\n",
    "# or save to csv: \n",
    "hist_csv_file = 'EffNet_b1_80_epoch_history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = 'C:/Users/Monir/Documents/CSE499/results_and_figures/EfficientNet/B1/'\n",
    "\n",
    "import tikzplotlib\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(EPOCHS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyVUlEQVR4nO3deZhU9ZX/8fex2ReRTUV2DC6sDbQY0URw1/gT1xEGE9AoakiMxBh1kolEh9HJ4xhHo864m2gkuAb3hahETURARFCJKCgtyKZsKt10c35/fG91F011d3X3ra6q5vN6nvtU1a17b526VXVPne/3LubuiIiIAOyR7QBERCR3KCmIiEgFJQUREamgpCAiIhWUFEREpIKSgoiIVFBSkIwws2fNbGLc02aTma0ws2MysNxXzOz86P4EM3shnWnr8Tq9zGyrmRXUN1Zp+pQUpEK0wUgMO8zsm6THE+qyLHc/0d3vj3vaXGRmV5nZnBTju5hZqZkNSndZ7v6gux8XU1w7JTF3/9Td27l7eRzLr/Jabmbfinu50viUFKRCtMFo5+7tgE+B/5c07sHEdGbWLHtR5qQ/AqPMrG+V8eOAd919cRZiEqkXJQWplZmNNrNiM7vCzD4H7jWzjmb2lJmtM7Mvo/s9kuZJbhKZZGavmdkN0bTLzezEek7b18zmmNkWM3vJzG41sweqiTudGK81s9ej5b1gZl2Snv++mX1iZhvM7JfVrR93Lwb+Cny/ylM/AO6vLY4qMU8ys9eSHh9rZh+Y2SYz+z1gSc/tb2Z/jeJbb2YPmtle0XN/BHoBT0aV3i/MrE/0j75ZNM1+ZjbLzL4ws2VmdkHSsqeZ2Uwz+0O0bpaYWVF166A6ZtYhWsa6aF3+ysz2iJ77lpm9Gr239Wb252i8mdnvzGxt9NyiulRb0jBKCpKufYFOQG9gMuG7c2/0uBfwDfD7GuY/FFgKdAF+C9xtZlaPaf8EzAU6A9PYdUOcLJ0Y/xU4F9gbaAH8HMDMBgC3R8vfL3q9lBvyyP3JsZjZgUAh8FCacewiSlCPAr8irIuPgMOTJwGui+I7GOhJWCe4+/fZudr7bYqXeAgojuY/E/hPMzs66flTgBnAXsCsdGJO4RagA9APOJKQKM+NnrsWeAHoSFi3t0TjjwO+CxwQvfbZwIZ6vLbUh7tr0LDLAKwAjonujwZKgVY1TF8IfJn0+BXg/Oj+JGBZ0nNtAAf2rcu0hA1qGdAm6fkHgAfSfE+pYvxV0uMfAc9F938NzEh6rm20Do6pZtltgM3AqOjxdOAv9VxXr0X3fwD8I2k6I2zEz69muacCb6f6DKPHfaJ12YyQQMqB9knPXwfcF92fBryU9NwA4Jsa1q0D36oyrgAoAQYkjbsQeCW6/wfgDqBHlfmOAv4JfBvYI9u/hd1tUKUg6Vrn7tsSD8ysjZn9X9QksBmYA+xl1e/Z8nnijrt/Hd1tV8dp9wO+SBoHsLK6gNOM8fOk+18nxbRf8rLd/Stq+LcaxfQw8IOoqplAqB7qs64SqsbgyY/NbG8zm2Fmn0XLfYBQUaQjsS63JI37BOie9LjqumlldetP6kKovj6p5jV+QUh0c6PmqfMA3P2vhKrkVmCNmd1hZnvW4XWlAZQUJF1VT6d7GXAgcKi770ko9yGpzTsDVgOdzKxN0rieNUzfkBhXJy87es3OtcxzP/AvwLFAe+CpBsZRNQZj5/d7HeFzGRIt95wqy6zpFMirCOuyfdK4XsBntcRUF+uB7YRms11ew90/d/cL3H0/QgVxm0V7MLn7ze4+AhhIaEa6PMa4pAZKClJf7Qlt4xvNrBNwdaZf0N0/AeYB08yshZkdBvy/DMX4CHCymR1hZi2Aa6j99/I3YCOhSWSGu5c2MI6ngYFmdnr0D/0SQjNaQntga7Tc7uy64VxDaMvfhbuvBN4ArjOzVmY2BPgh8GCq6dPUIlpWKzNrFY2bCUw3s/Zm1hv4GaGiwczOSupw/5KQxMrN7BAzO9TMmgNfAdsITV3SCJQUpL5uAloT/g3+A3iukV53AnAYoSnnP4A/E9qtU7mJesbo7kuAKYSO7dWEjVZxLfM4oZ28d3TboDjcfT1wFnA94f32B15PmuQ3wHBgEyGBPFZlEdcBvzKzjWb28xQvMZ7Qz7AKeBy42t1fTCe2aiwhJL/EcC7wE8KG/WPgNcL6vCea/hDgTTPbSujI/qm7Lwf2BO4krPNPCO/9hgbEJXVgUceOSF6KdmP8wN0zXqmI7A5UKUheiZoW9jezPczsBGAs8ESWwxJpMnRkquSbfQnNJJ0JzTkXu/vb2Q1JpOlQ85GIiFRQ85GIiFTI6+ajLl26eJ8+fbIdhohIXpk/f/56d++a6rm8Tgp9+vRh3rx52Q5DRCSvmNkn1T2n5iMREamgpCAiIhWUFEREpEJe9ymISOPZvn07xcXFbNu2rfaJJSe0atWKHj160Lx587TnUVIQkbQUFxfTvn17+vTpQ/XXR5Jc4e5s2LCB4uJi+vateqXY6qn5SETSsm3bNjp37qyEkCfMjM6dO9e5slNSEJG0KSHkl/p8XkoKGTBjBmzcmO0oRETqTkkhZmvXwvjxITGISDw2bNhAYWEhhYWF7LvvvnTv3r3icWlpaY3zzps3j0suuaTW1xg1alQssb7yyiucfPLJsSwrG9TRHLOvo6sHb9lS83QiTd2DD8Ivfwmffgq9esH06TBhQv2W1blzZxYuXAjAtGnTaNeuHT//eeV1g8rKymjWLPXmrKioiKKiolpf44033qhfcE2MKoWYlUTXAPvmm+zGIZJNDz4IkyfDJ5+Ae7idPDmMj8ukSZP42c9+xpgxY7jiiiuYO3cuo0aNYtiwYYwaNYqlS5cCO/9znzZtGueddx6jR4+mX79+3HzzzRXLa9euXcX0o0eP5swzz+Sggw5iwoQJJM4m/cwzz3DQQQdxxBFHcMkll9SpInjooYcYPHgwgwYN4oorrgCgvLycSZMmMWjQIAYPHszvfvc7AG6++WYGDBjAkCFDGDduXMNXVh2oUohZopJVUpDd2S9/WVk1J3z9dRhf32ohlX/+85+89NJLFBQUsHnzZubMmUOzZs146aWX+Ld/+zceffTRXeb54IMPePnll9myZQsHHnggF1988S778b/99tssWbKE/fbbj8MPP5zXX3+doqIiLrzwQubMmUPfvn0ZP3582nGuWrWKK664gvnz59OxY0eOO+44nnjiCXr27Mlnn33G4sWLAdgYdUZef/31LF++nJYtW1aMayyqFGKmSkEkNBnVZXx9nXXWWRQUFACwadMmzjrrLAYNGsTUqVNZsmRJynm+973v0bJlS7p06cLee+/NmjVrdplm5MiR9OjRgz322IPCwkJWrFjBBx98QL9+/Sr2+a9LUnjrrbcYPXo0Xbt2pVmzZkyYMIE5c+bQr18/Pv74Y37yk5/w3HPPseeeewIwZMgQJkyYwAMPPFBts1imKCnETJWCSOhDqMv4+mrbtm3F/X//939nzJgxLF68mCeffLLa/fNbtmxZcb+goICysrK0pmnIBcmqm7djx4688847jB49mltvvZXzzz8fgKeffpopU6Ywf/58RowYkTLGTFFSiJkqBZHQqdymzc7j2rQJ4zNl06ZNdO/eHYD77rsv9uUfdNBBfPzxx6xYsQKAP//5z2nPe+ihh/Lqq6+yfv16ysvLeeihhzjyyCNZv349O3bs4IwzzuDaa69lwYIF7Nixg5UrVzJmzBh++9vfsnHjRrZu3Rr7+6mO+hRipkpBpLLfIK69j9Lxi1/8gokTJ3LjjTdy1FFHxb781q1bc9ttt3HCCSfQpUsXRo4cWe20s2fPpkePHhWPH374Ya677jrGjBmDu3PSSScxduxY3nnnHc4991x27NgBwHXXXUd5eTnnnHMOmzZtwt2ZOnUqe+21V+zvpzp5fY3moqIiz7WL7MyaBWPHwve+B089le1oROLz/vvvc/DBB2c7jKzaunUr7dq1w92ZMmUK/fv3Z+rUqdkOq0apPjczm+/uKffTVfNRzBKVQtU9L0Qk/915550UFhYycOBANm3axIUXXpjtkGKn5qOYqU9BpOmaOnVqzlcGDaVKIWbqUxCRfJaxpGBmrcxsrpm9Y2ZLzOw30fhpZvaZmS2MhpOS5rnKzJaZ2VIzOz5TsWWSKgURyWeZbD4qAY5y961m1hx4zcyejZ77nbvfkDyxmQ0AxgEDgf2Al8zsAHcvz2CMsVOlICL5LGOVggeJnWubR0NNuzqNBWa4e4m7LweWAdXv85WjVCmISD7LaJ+CmRWY2UJgLfCiu78ZPfVjM1tkZveYWcdoXHdgZdLsxdG4qsucbGbzzGzeunXrMhl+vahSEInf6NGjef7553cad9NNN/GjH/2oxnkSu6yfdNJJKc8hNG3aNG644YZdxid74okneO+99yoe//rXv+all16qQ/Sp5eoptjOaFNy93N0LgR7ASDMbBNwO7A8UAquB/44mT3WJoF0qC3e/w92L3L2oa9euGYm7IZIrhTw+BEQkp4wfP54ZVS5SMmPGjLTPP/TMM8/U+wCwqknhmmuu4ZhjjqnXsvJBo+x95O4bgVeAE9x9TZQsdgB3UtlEVAz0TJqtB7CqMeKLU6JS2LGj8r6INMyZZ57JU089RUn0r2vFihWsWrWKI444gosvvpiioiIGDhzI1VdfnXL+Pn36sH79egCmT5/OgQceyDHHHFNxem0IxyAccsghDB06lDPOOIOvv/6aN954g1mzZnH55ZdTWFjIRx99xKRJk3jkkUeAcOTysGHDGDx4MOedd15FfH369OHqq69m+PDhDB48mA8++CDt95rtU2xnrKPZzLoC2919o5m1Bo4B/svMurn76miy04DF0f1ZwJ/M7EZCR3N/YG6m4suURKUAoVpIOq+WSJNx6aUQXfMmNoWFcNNNqZ/r3LkzI0eO5LnnnmPs2LHMmDGDs88+GzNj+vTpdOrUifLyco4++mgWLVrEkCFDUi5n/vz5zJgxg7fffpuysjKGDx/OiBEjADj99NO54IILAPjVr37F3XffzU9+8hNOOeUUTj75ZM4888ydlrVt2zYmTZrE7NmzOeCAA/jBD37A7bffzqWXXgpAly5dWLBgAbfddhs33HADd911V63rIBdOsZ3JSqEb8LKZLQLeIvQpPAX81szejcaPAaYCuPsSYCbwHvAcMCXf9jyCnasD9SuIxCe5CSm56WjmzJkMHz6cYcOGsWTJkp2aeqr629/+xmmnnUabNm3Yc889OeWUUyqeW7x4Md/5zncYPHgwDz74YLWn3k5YunQpffv25YADDgBg4sSJzJkzp+L5008/HYARI0ZUnESvNrlwiu2MVQruvggYlmL892uYZzqQwfMoZl7VSkGkKaruH30mnXrqqfzsZz9jwYIFfPPNNwwfPpzly5dzww038NZbb9GxY0cmTZpU7SmzE8xSdV+GK7k98cQTDB06lPvuu49XXnmlxuXUdt64xOm3qzs9d12WmTjF9vPPP8+tt97KzJkzueeee3j66aeZM2cOs2bN4tprr2XJkiUNTg46ojlmqhREMqNdu3aMHj2a8847r6JK2Lx5M23btqVDhw6sWbOGZ599tsZlfPe73+Xxxx/nm2++YcuWLTz55JMVz23ZsoVu3bqxfft2Hky6bmj79u3ZkuKi6wcddBArVqxg2bJlAPzxj3/kyCOPbNB7zIVTbOvcRzFTpSCSOePHj+f000+vaEYaOnQow4YNY+DAgfTr14/DDz+8xvmHDx/O2WefTWFhIb179+Y73/lOxXPXXnsthx56KL1792bw4MEViWDcuHFccMEF3HzzzRUdzACtWrXi3nvv5ayzzqKsrIxDDjmEiy66qE7vJxdPsa1TZ8fstNPgiSfC/TlzIOk7J5LXdOrs/KRTZ2eZKgURyWdKCjErLYXWrcN9XVNBRPKNkkLMSkog0aynSkGamnxubt4d1efzUlKImZKCNFWtWrViw4YNSgx5wt3ZsGEDrVq1qtN82vsoZqWlSgrSNPXo0YPi4mJy8USUklqrVq122rspHUoKMSspgW7dwn0lBWlKmjdvTt++fbMdhmSYmo9iVloKHTqE+0oKIpJvlBRiVlICbdtCs2ZKCiKSf5QUYlZaCi1ahN1SlRREJN8oKcSspCScLrt1ax2nICL5R0khZqoURCSfKSnEyL2yUmjTRklBRPKPkkKMystDYkg0HykpiEi+UVKIUeJkeGo+EpF8lbGkYGatzGyumb1jZkvM7DfR+E5m9qKZfRjddkya5yozW2ZmS83s+EzFlimJC+yoUhCRfJXJSqEEOMrdhwKFwAlm9m3gSmC2u/cHZkePMbMBwDhgIHACcJuZFWQwvtipUhCRfJexpOBB4tpwzaPBgbHA/dH4+4FTo/tjgRnuXuLuy4FlwMhMxZcJqhREJN9ltE/BzArMbCGwFnjR3d8E9nH31QDR7d7R5N2BlUmzF0fjqi5zspnNM7N5uXZirqqVgo5TEJF8k9Gk4O7l7l4I9ABGmtmgGia3VItIscw73L3I3Yu6du0aU6TxUKUgIvmuUfY+cveNwCuEvoI1ZtYNILpdG01WDPRMmq0HsKox4otLcqWg4xREJB9lcu+jrma2V3S/NXAM8AEwC5gYTTYR+Et0fxYwzsxamllfoD8wN1PxZUKqSkHXIxGRfJLJ6yl0A+6P9iDaA5jp7k+Z2d+BmWb2Q+BT4CwAd19iZjOB94AyYIq7l2cwvthV7VPYsQO2bw+PRUTyQcaSgrsvAoalGL8BOLqaeaYD0zMVU6ZVrRQgVAtKCiKSL3REc4yqVgqgfgURyS9KCjGqrlIQEckXSgoxSlUp6FgFEcknSgoxUqUgIvlOSSFG6lMQkXynpBCj5EqhTZtwX0lBRPKJkkKMVCmISL5TUoiR+hREJN8pKcQoUSk0b66kICL5SUkhRqWlISHssYeSgojkJyWFGJWUVJ7SQscpiEg+UlKIUUlJ6E8AVQoikp+UFGJUWlqZFJo3h4ICJQURyS9KCjFKbj4CXWhHRPKPkkKMkisF0CU5RST/KCnEqGqloKQgIvlGSSFGqhREJN8pKcRIlYKI5LuMJQUz62lmL5vZ+2a2xMx+Go2fZmafmdnCaDgpaZ6rzGyZmS01s+MzFVumpKoUdJyCiOSTjF2jGSgDLnP3BWbWHphvZi9Gz/3O3W9IntjMBgDjgIHAfsBLZnaAu5dnMMZYlZRAhw6Vj5UURCTfZKxScPfV7r4gur8FeB/oXsMsY4EZ7l7i7suBZcDITMWXCepTEJF81yh9CmbWBxgGvBmN+rGZLTKze8ysYzSuO7AyabZiUiQRM5tsZvPMbN66desyGXadqU9BRPJdxpOCmbUDHgUudffNwO3A/kAhsBr478SkKWb3XUa43+HuRe5e1LVr18wEXU9VKwUdvCYi+SajScHMmhMSwoPu/hiAu69x93J33wHcSWUTUTHQM2n2HsCqTMYXN1UKIpLvMrn3kQF3A++7+41J47slTXYasDi6PwsYZ2Ytzawv0B+Ym6n4MkF9CiKS7zK599HhwPeBd81sYTTu34DxZlZIaBpaAVwI4O5LzGwm8B5hz6Up+bTnEVRfKbiDpWocExHJMRlLCu7+Gqn7CZ6pYZ7pwPRMxZRpqSqF8nLYvn3nZCEikqt0RHOMUlUKoCYkEckfSgoxKS8PQ9VKAZQURCR/KCnEpLQ03KpSEJF8pqQQk0RSqHqcAigpiEj+UFKISUlJuFWlICL5TEkhJqkqBSUFEck3SgoxUaUgIk2BkkJMaqoUdPpsEckXSgoxUaUgIk2BkkJM1KcgIk2BkkJMEpWCkoKI5DMlhZikOnhNxymISL5RUoiJKgURaQqUFGKSqlJo3hwKCpQURCR/KCnEJFWlALrQjojkFyWFmKTaJRVCUtBxCiKSL5QUYpJql1RQpSAi+SWtpGBmbc1sj+j+AWZ2ipk1r2Wenmb2spm9b2ZLzOyn0fhOZvaimX0Y3XZMmucqM1tmZkvN7PiGvLHGVlOloKQgIvki3UphDtDKzLoDs4FzgftqmacMuMzdDwa+DUwxswHAlcBsd+8fLetKgOi5ccBA4ATgNjMrqNvbyR5VCiLSFKSbFMzdvwZOB25x99OAATXN4O6r3X1BdH8L8D7QHRgL3B9Ndj9wanR/LDDD3UvcfTmwDBhZh/eSVaoURKQpSDspmNlhwATg6Whcs3RfxMz6AMOAN4F93H01hMQB7B1N1h1YmTRbcTSu6rImm9k8M5u3bt26dEPIuFS7pEI4gE1JQUTyRbpJ4VLgKuBxd19iZv2Al9OZ0czaAY8Cl7r75pomTTHOdxnhfoe7F7l7UdeuXdMJoVGUlIRjEgqqNHipUhCRfJLWv313fxV4FSDqcF7v7pfUNl/UGf0o8KC7PxaNXmNm3dx9tZl1A9ZG44uBnkmz9wBWpfc2sq+0dNf+BFBSEJH8ku7eR38ysz3NrC3wHrDUzC6vZR4D7gbed/cbk56aBUyM7k8E/pI0fpyZtTSzvkB/YG76byW7Skp2bToCHacgIvkl3eajAVHTz6nAM0Av4Pu1zHN4NM1RZrYwGk4CrgeONbMPgWOjx7j7EmAmIek8B0xx9/I6vp+sUaUgIk1Bup3FzaOmoFOB37v7djPbpb0/mbu/Rup+AoCjq5lnOjA9zZhySk2VgpKCiOSLdCuF/wNWAG2BOWbWG6ip03i3o0pBRJqCtJKCu9/s7t3d/SQPPgHGZDi2vFJTpVBeDtu3N35MIiJ1lW5HcwczuzFxfICZ/TehapBIdZWCLrQjIvkk3eaje4AtwL9Ew2bg3kwFlY9qqhRASUFE8kO6Hc37u/sZSY9/Y2YLMxBP3qqpTwGUFEQkP6RbKXxjZkckHpjZ4YA2c0lUKYhIU5BupXAR8Acz6xA9/pLKA9CEUCl06rTr+ERS0AFsIpIP0j3NxTvAUDPbM3q82cwuBRZlMLa8UlKi5iMRyX91uvKau29OOqndzzIQT94qLU3dfNQ22kdr69bGjUdEpD4acjnO6o5W3i1VVykkmpS++KJx4xERqY+GJIUaT3Oxu6muUujcOdxu2NC48YiI1EeNfQpmtoXUG38DWmckojxVW6WgpCAi+aDGpODu7RsrkHxXXaXQrBl06KCkICL5oSHNR5KkukoBQhOS+hREJB8oKcTAPZzwLlWlACEpqFIQkXygpBCD0tJwW1OloKQgIvlASSEGiaSgSkFE8l3GkoKZ3WNma81scdK4aWb2WZXLcyaeu8rMlpnZUjM7PlNxZUJJSbhVpSAi+S6TlcJ9wAkpxv/O3Quj4RkAMxsAjAMGRvPcZmYFGYwtVulUCps360I7IpL7MpYU3H0OkO4+N2OBGe5e4u7LgWXAyEzFFrd0KgXQHkgikvuy0afwYzNbFDUvdYzGdQdWJk1THI3bhZlNTlwBbt26dZmONS2JpFBTpQBqQhKR3NfYSeF2YH+gEFgN/Hc0PtV5lFKeRsPd73D3Incv6tq1a0aCrKva9j7SUc0iki8aNSm4+xp3L3f3HcCdVDYRFQM9kybtAaxqzNgaQpWCiDQVjZoUzKxb0sPTgMSeSbOAcWbW0sz6Av2BuY0ZW0Okc5wCKCmISO5L98prdWZmDwGjgS5mVgxcDYw2s0JC09AK4EIAd19iZjOB94AyYIq7l2cqtrilWymoo1lEcl3GkoK7j08x+u4app8OTM9UPJlUW6XQrh00b65KQURyn45ojkFtlYKZDmATkfygpBCD2ioFUFIQkfygpBCD2ioFUFIQkfygpBADVQoi0lQoKcSgttNcgJKCiOQHJYUY1HZCPKhMCp7yOG0RkdygpBCDdCuF7dth69bGiUlEpD6UFGKQbqUAakISkdympBCDkhLYYw9oVsOhgDopnojkAyWFGJSW1lwlgE51ISL5QUkhBiUlNfcngJqPRCQ/KCnEoC6VgpKCiOQyJYUYpFMpqE9BRPKBkkIM0qkUmjeHPfdUUhCR3KakEIN0KgXQUc0ikvuUFGKQTqUASgoikvuUFGKgSkFEmgolhRioUhCRpiJjScHM7jGztWa2OGlcJzN70cw+jG47Jj13lZktM7OlZnZ8puLKBFUKItJUZLJSuA84ocq4K4HZ7t4fmB09xswGAOOAgdE8t5lZQQZji1VdKoVNm6CsLPMxiYjUR8aSgrvPAaqe1GEscH90/37g1KTxM9y9xN2XA8uAkZmKLW51qRQAvvwys/GIiNRXY/cp7OPuqwGi272j8d2BlUnTFUfjdmFmk81snpnNW7duXUaDTVe6lYIOYBORXJcrHc2WYlzKy9G4+x3uXuTuRV27ds1wWOmpa6WgpCAiuaqxk8IaM+sGEN2ujcYXAz2TpusBrGrk2OqtLn0KoKQgIrmrsZPCLGBidH8i8Jek8ePMrKWZ9QX6A3MbObZ6U6UgIk1FDZeFaRgzewgYDXQxs2LgauB6YKaZ/RD4FDgLwN2XmNlM4D2gDJji7uWZii1u27YpKYhI05CxpODu46t56uhqpp8OTM9UPJmybVu47nKXLrVP2759uDqbkoKI5Kpc6WjOW4kdoPbeu+bpAMx0AJuI5DYlhQZaG3WVp5MUQElBRHKbkkIDKSmISFOipNBA9UkKX1Q9zltEJEcoKTSQKgURaUqUFBpo7Vpo3Rratk1v+kRS8JTHa4uIZJeSQgOtXRuqBEt1oo4UOncOB7t9/XVm4xIRqQ8lhQZKJIV07btvuP3448zEIyLSEEoKDVTXpHDMMeF21qzMxCMi0hBKCg1U16Sw335w2GHw6KOZi0lEpL6UFBrAve5JAeCMM+Dtt2H58szEJSJSX0oKDbB5czhtdl2Twumnh9vHHos/JhGRhlBSaIC6HqOQ0LcvDBumpCAiuUdJoQHqmxQgVAtvvAGr8uZSQiKyO1BSaICGJIUzzgi3TzwRWzgiIg2mpNAADUkKBx8MBx2kvZBEJLcoKTRAIimkc4GdVM44A159Fdavjy8mEZGGyEpSMLMVZvaumS00s3nRuE5m9qKZfRjddsxGbHWxdi107AgtWtRv/tNPh/JyHcgmIrkjm5XCGHcvdPei6PGVwGx37w/Mjh7ntPoco5Bs2DDo0wdmzAjJQUQk23Kp+WgscH90/37g1OyFkp6GJgUzOOccePFF6NkTfv5zWLhQZ1AVkezJVlJw4AUzm29mk6Nx+7j7aoDoNuXm1swmm9k8M5u3LnGB5CxpaFIAuPpqePhhGDkSbr45VA8//GE88YmI1FW2ksLh7j4cOBGYYmbfTXdGd7/D3Yvcvahr166ZizANcSSFZs3gzDPDrqmrV8O//Av86U/hSGkRkcaWlaTg7qui27XA48BIYI2ZdQOIbtdmI7Z0lZWFi+U0NCkk69w5JIWSknBuJBGRxtboScHM2ppZ+8R94DhgMTALmBhNNhH4S2PHVheJq6fFmRQgnEEV4O9/j3e5IiLpyEalsA/wmpm9A8wFnnb354DrgWPN7EPg2OhxzkocoxB3C9Z++0GvXkoKIpIdzRr7Bd39Y2BoivEbgKMbO576asjRzLUZNQpefz3+5YqI1CaXdknNK5lMCocdBitXQnFx/MsWEamJkkI9ZTopgJqQRKTxKSnU09q1UFAQTnMRt6FDoVWrmpPCu+/C5ZfDgAHhiGgRkTg0ep9CU7F2behk3iMDabVFCzjkkNRJ4f774aabwpHPzZqFjulzzoHWrWHs2PhjEZHdiyqFeorjwLWaHHYYzJ8P27ZVjlu4ECZNCrvC3nJLuEDP4sVQVBSOb3jhhczFIyK7B1UK9dQYSeG3v4UFC8LeSADXXAMdOsArr8Bee1VO++yzMGYMnHoqPP88fOc7Oy9r5cpQdfz97+E03YMHQ2FhGDL5HkQk/ygp1NPatdCvX+aWn9zZPGoULFoEjz8Ov/71zgkBQr/GCy/Ad78Lxx0H3buHZq2CAti0KZw+A0ITU6dO8MADlfMefDBcdBFMnBgSjojs3pQU6inTlcI++4Skk+hXuPZaaN8eLr009fR77w2zZ8P06bB5czgVd3l56LA+5JCQZIYOhebN4Ysv4J13QhXy8MPw05/CVVeFvolx48LJ+dq2zdx7E5HcpaRQD19/DVu3Zr7p5bDD4K9/Df0GjzwCv/xlzXs7de8Ot91W+3I7dQrNTWPGwGWXhb6L226DP/wB7rgjVBjDhoUK5aKLQjUhIrsHdTTXQ+KM3Y2RFFavDhvmdu1g6tTMvM6IEXD33fD55/DMM3DlleH17rwzJIebboIdOzLz2pmg61E0XTt2hO/p7v4ZL18e+gozQZVCPWTywLVkiX6F118PzTudO2f29Tp0gBNPDAPAmjVwwQUhGT35JNx7bzgvUzpWrw7rqVOnMLRpE84s+8kn8PHH4Ut9wAFw5JF12633q68qfxBr14YEvXZteL2VKyuPBP/Wt2DatHAd7EzsNtyUbd8Of/sbfPNN2OW5e/dwHfK416N7OEX8tm3hglPNm4fdsQsKdp1u0SJ46KEwfPop9O4NJ5wQvqsjRoTvwKpVYXCHgQPDkInjiLLtscfgvPNCJf/MM/Ev3zyPU25RUZHPmzev0V/36afh5JPhH/+AQw/N3OuUlYUNtRmsWBF+mI3NPVQRU6dWnhV269YwlJaGK8btv38Y9twzHFS3cGFIKMlatgwbm6oVx/77h4sKTZoE3bqlfv3HHoMbb4QPP6ys0pK1aAH77hti6dkzbMSefRbeey/0o1xzTdhAfPRRSEgrV4ZO+WOPrfuGLvFzMdt5fHl5eO//+EfYEeCQQ0KfUNXpkm3eHP719u9f83SZUFYWdkJwD8OOHeF07Q8/HK7t8cUXO0/fokVYZ//6r+Ha4h06hHnmzg3XGH/zzfAZ9OkDffuG5z/8EJYuDcPnn4fvS2lp+B6UlIQhFbPQp9W2bahYy8vD979Zs7AjxZFHhr62l14K38OadOsWPufLLoMhQ6qfbt06uO++sJEdNCi8zujRoR9v27bwnX7rrfCHJLETR0FB+P6efXaIM12lpWGnkeeeg8MPD+uzU6fw3I4d8PLLcM898NlnMGECjB8fll9SEg5YveWW0O/35z+H9V0fZjY/6VLIOz+npFB3994bMvXHH4cfQCb95jfhx3bhhZl9ndp8/DH853+GL3S7dmEoKAj//D/6KAybN4cfVGJ31/32g40bwwZmw4awYUkkkF694LXX4K67wi62BQVwyikwZQocdVTYMKxcCT/+cdjoDBgQfkB9+4ahV6/QGd+1a/jhptpIz5gRqoVly1K/pwMOCK83cWLYQL77bhiWLg0biQ0bwi68GzeGH+S2bWGD1qJFSD69eoXbNWvgjTdgy5adl9+xY/gX26dP+Az33Test/nzw3t/552wERg8OHy+55wTNqYlJZXTfPJJ5YZ0+/awfs87L7z3BPewwXrkkbAuEut4333D/IkN87JlYUNTXBxiTvXTb98+fA5nnBE+v88+C8Py5fCXv4TvQSJBLFpUeWR/YWFYXytX7ny98e7d4cADw3pq0SJUA4mKoHXrsCNEy5Zh2uSE8c03lX8+tm8P34kzz9z5j1FpaVjv778f3muiqikvhyVLwpDYa++rr0Jlcfnl4Y/Cpk3hc129OuyN99hjYXmDB4fv8tdfhyS0//7hcVlZeM02bSq/X+XllX/czj0XfvSjEN+cOeE7/dpr4bkRI8LQv394nTvvDOu/Xbvw/hLJbsiQsKFfvjz8sdh3X/jgg/CZnHNOSMDz54c/aNdfH9ZhfSkpxOy//iu0u2/dqr10krnX7x/vhx+G5HD33WHDctBB4Qd8111ho3nNNWEPqWb1aOwsK4NHHw0/8n79wo+8S5fw4/z971MfNd6tW/hBdukShr32qtx4tWwZNlgrV4ZmjE8/DT/8I44Iw2GHhQ3OvHlhWLAgbITXrq2sktq0CdMdcURoErzvvjBdmzZho7RwYeW/6E6dwmu2aBH+oS5fHjaqZ54J558fNhp33BESTLNmlRuvqlq1Cu+9R4+w4ezRIyx7jz3CZ2YWktcxx1RupKtyDxumhx4Ku0APGRISyIknVjbTlJWFdbNpU3i99u3r/pnF7csv4fbb4X/+p7LpN9lee4U/BpMnhz8fJSUh2bzwQkgsgwaFyu+QQ8K6S3zH3UNleMstocIqKwvPuYf1/e1vhz8KixaFxAbh+e99LySQ448Pn9uMGWH49FM4+uhQOZ96auWpbv73f2HmzJBE77svnjMXKCnE7LLLwgf11VeN/tJN2rZt4Z/SrbeGf77HHx9+zJmsxhYsCAmie/ewQR40aNfjQOJQXl5ZdfTrFzbsyebNg//7v/CPfuTIkDBGjdq132rp0vDdu/fesOGFsDPA5MmhaadZs9DU8tFH4V9w797hn3qvXupb2bYtbLy//DIk8g4dQjIbOTJscBti9eqwwd6+PTQ7HXpoZXItKQl7EL73XvhcU32f3UOlXd2xQps2hYqsLs1UNakpKeDueTuMGDHC6+OBB9x793Y3c+/cOQxmYdzFF7t37x5aWrt2dT/+ePcuXcLjXr3C823bhseJ6RPLqu1xqtdKd96GLvuBB6pfB7n0PhKPe/QI67jqa9X2Pur6fEPWSV3XafL0DVn/vXu7n39+5feyts8nzs8+7teqy/Rxfxdy5bda03eyPuswHcA8r2a7mvUNe0OG+iSFBx5wb9Mm0b22+wzNm1d+ETt3dm/RIvsxZeJ91OX55B8YhHGZXKeJ6evzWvny2dd3ndQntoZ+F3JlqO47mW7cbdrUPTHUlBRyrvnIzE4A/gcoAO5y92ovy1mf5qM+fULnm4hIU9G7d2g2TFdNzUc51cpoZgXArcCJwABgvJkNiPM1Pv00zqWJiGRfnNu1nEoKwEhgmbt/7O6lwAxgbJwvkO7BVyIi+SLO7VquJYXuQPLB28XRuNhMn165r7GISL5r0yZs1+KSa0kh1V7uO3V6mNlkM5tnZvPWpTq8tRYTJoT9unv3DvsMd+4cBrMw7uKLK5+L+3EmX6umZXfuvOuBLs2b1z+WbK2z2t5HfZ6vKrEPejpx1WWdppo+3dfK5DrL5mvVZ/q4vgu58ltN5ztZW9x33BG2a7HJ1J5B9RmAw4Dnkx5fBVxV3fT13SV1d1SXXTNzWUN3QY1z9766rtNsfQaN+bqNuU7i3B05mzKxy2ltyJe9j8ysGfBP4GjgM+At4F/dfUmq6bN18JqISD6rae+jnDpLqruXmdmPgecJu6TeU11CEBGR+OVUUgBw92eADJwQVkREapNrHc0iIpJFSgoiIlJBSUFERCrk1N5HdWVm64CGnMmoC7A+pnDilKtxQe7GlqtxQe7GlqtxQe7GlqtxQd1i6+3uXVM9kddJoaHMbF51u2VlU67GBbkbW67GBbkbW67GBbkbW67GBfHFpuYjERGpoKQgIiIVdvekcEe2A6hGrsYFuRtbrsYFuRtbrsYFuRtbrsYFMcW2W/cpiIjIznb3SkFERJIoKYiISIXdMimY2QlmttTMlpnZlVmO5R4zW2tmi5PGdTKzF83sw+i2Yxbi6mlmL5vZ+2a2xMx+mguxmVkrM5trZu9Ecf0mF+KqEmOBmb1tZk/lUmxmtsLM3jWzhWY2L1diM7O9zOwRM/sg+r4dliNxHRitq8Sw2cwuzZHYpkbf/8Vm9lD0u4glrt0uKTTGdaDr6D7ghCrjrgRmu3t/YHb0uLGVAZe5+8HAt4Ep0XrKdmwlwFHuPhQoBE4ws2/nQFzJfgq8n/Q4l2Ib4+6FSfuz50Js/wM85+4HAUMJ6y7rcbn70mhdFQIjgK+Bx7Mdm5l1By4Bitx9EOGM0uNii6u6Cy001YE6XsinkWLqAyxOerwU6Bbd7wYszYH19hfg2FyKDWgDLAAOzZW4gB7RD/Io4Klc+jyBFUCXKuOyGhuwJ7CcaKeXXIkrRZzHAa/nQmxUXra4E+FM109F8cUS125XKdAI14GOwT7uvhogut07m8GYWR9gGPAmORBb1DyzEFgLvOjuORFX5CbgF8COpHG5EpsDL5jZfDObnCOx9QPWAfdGTW53mVnbHIirqnHAQ9H9rMbm7p8BNwCfAquBTe7+Qlxx7Y5JodbrQEslM2sHPApc6u6bsx0PgLuXeyjpewAjzWxQlkMCwMxOBta6+/xsx1KNw919OKHpdIqZfTfbARH+6Q4Hbnf3YcBXZLd5bRdm1gI4BXg427EARH0FY4G+wH5AWzM7J67l745JoRjomfS4B7AqS7FUZ42ZdQOIbtdmIwgza05ICA+6+2O5FBuAu28EXiH0yeRCXIcDp5jZCmAGcJSZPZAjseHuq6LbtYS28ZE5EFsxUBxVewCPEJJEtuNKdiKwwN3XRI+zHdsxwHJ3X+fu24HHgFFxxbU7JoW3gP5m1jf6BzAOmJXlmKqaBUyM7k8ktOc3KjMz4G7gfXe/MVdiM7OuZrZXdL814QfyQbbjAnD3q9y9h7v3IXyv/uru5+RCbGbW1szaJ+4T2qAXZzs2d/8cWGlmB0ajjgbey3ZcVYynsukIsh/bp8C3zaxN9Ds9mtA5H09c2ey8ydYAnAT8E/gI+GWWY3mI0C64nfCv6YdAZ0Jn5YfRbacsxHUEoVltEbAwGk7KdmzAEODtKK7FwK+j8VlfZ1XiHE1lR3PWYyO03b8TDUsS3/scia0QmBd9pk8AHXMhrii2NsAGoEPSuKzHBvyG8GdoMfBHoGVccek0FyIiUmF3bD4SEZFqKCmIiEgFJQUREamgpCAiIhWUFEREpIKSgkgKZlZe5QyZsR1la2Z9LOmsuCK5pFm2AxDJUd94OJWGyG5FlYJIHUTXJPiv6JoOc83sW9H43mY228wWRbe9ovH7mNnjFq7/8I6ZjYoWVWBmd0bnxH8hOjobM7vEzN6LljMjS29TdmNKCiKpta7SfHR20nOb3X0k8HvCWVGJ7v/B3YcADwI3R+NvBl71cP2H4YSjiQH6A7e6+0BgI3BGNP5KYFi0nIsy89ZEqqcjmkVSMLOt7t4uxfgVhIv8fBydMPBzd+9sZusJ57LfHo1f7e5dzGwd0MPdS5KW0Ydwyu/+0eMrgObu/h9m9hywlXC6hyfcfWuG36rITlQpiNSdV3O/umlSKUm6X05l/973CFcGHAHMNzP1+0mjUlIQqbuzk27/Ht1/g3BmVIAJwGvR/dnAxVBxcaA9q1uome0B9HT3lwkX6tkL2KVaEckk/QsRSa11dHW3hOfcPbFbaksze5Pwp2p8NO4S4B4zu5xwJbFzo/E/Be4wsx8SKoKLCWfFTaUAeMDMOhAuBvU7D9eMEGk06lMQqYOoT6HI3ddnOxaRTFDzkYiIVFClICIiFVQpiIhIBSUFERGpoKQgIiIVlBRERKSCkoKIiFT4/3yfcniAnFNtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs_range, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.savefig(result_dir + 'b1_80_epoch_oss.png')\n",
    "plt.savefig(result_dir + 'b1_80_epoch_loss.pdf', dpi=150)\n",
    "tikzplotlib.save(result_dir + 'b1_80_epoch_loss.tex')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.8):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_range = range(15,EPOCHS) \n",
    "plt.plot(epochs_range,smooth_curve(loss[15:]), 'bo', label='Training loss')\n",
    "plt.plot(epochs_range,smooth_curve(val_loss[15:]), 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig(result_dir + 'b1_80_epoch_loss_smooth.png')\n",
    "plt.savefig(result_dir + 'b1_80_epoch_smooth.pdf', dpi=150)\n",
    "tikzplotlib.save(result_dir + 'b1_80_epoch_smooth.tex')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
